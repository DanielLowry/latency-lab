{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "991e4758",
   "metadata": {},
   "source": [
    "# Latency Lab Analysis\n",
    "\n",
    "This notebook reads `results/index.csv` and shows basic comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68561074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"scripts\" / \"results_lib.py\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "repo_root = find_repo_root(Path().resolve())\n",
    "sys.path.append(str(repo_root / \"scripts\"))\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from results_lib import load_index\n",
    "from analysis_utils import (\n",
    "    ensure_dataframe,\n",
    "    resolve_index_paths,\n",
    "    prepare_summary,\n",
    "    prepare_case,\n",
    "    build_profile,\n",
    "    plot_summary,\n",
    "    plot_case_distribution,\n",
    "    plot_profile,\n",
    ")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d22c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fd221f0",
   "metadata": {},
   "source": [
    "## Run benchmarks from the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f473a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"scripts\" / \"notebook_runner.py\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "repo_root = find_repo_root(Path().resolve())\n",
    "scripts_dir = repo_root / \"scripts\"\n",
    "if str(scripts_dir) not in sys.path:\n",
    "    sys.path.append(str(scripts_dir))\n",
    "\n",
    "import importlib\n",
    "import notebook_ui\n",
    "importlib.reload(notebook_ui)\n",
    "\n",
    "bench_path = repo_root / \"build\" / \"bench\"\n",
    "results_dir = repo_root / \"results\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc11bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui, output, runner = notebook_ui.display_runner_ui(\n",
    "    bench_path=bench_path,\n",
    "    results_dir=results_dir,\n",
    "    auto_load=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5dfbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = load_index(repo_root / \"results\" / \"index.csv\")\n",
    "index = resolve_index_paths(index, repo_root)\n",
    "index.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "0b6b4a380af943c8a1b512c5955a4737",
   "source": [
    "## Summary across cases (interactive)\n",
    "\n",
    "Use the controls below to select a metric, filter, and grouping. Labels are\n",
    "shortened in the plots to keep the charts readable.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "30cffb0848ce4605918dfec8fb4b536e",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "_summary_base = ensure_dataframe(index)\n",
    "_metric_candidates = [\"min\", \"p50\", \"p95\", \"p99\", \"p999\", \"max\", \"mean\"]\n",
    "metric_options = [m for m in _metric_candidates if m in _summary_base.columns]\n",
    "if not metric_options:\n",
    "    metric_options = [\"p50\"]\n",
    "\n",
    "_summary_hue_candidates = [\"pin_cpu\", \"tags\", \"bench_args\"]\n",
    "hue_options = [\"(none)\"] + [\n",
    "    col for col in _summary_hue_candidates if col in _summary_base.columns\n",
    "]\n",
    "\n",
    "lab_options = [\"(all)\"]\n",
    "if \"lab\" in _summary_base.columns:\n",
    "    labs = _summary_base[\"lab\"].dropna().astype(str).unique().tolist()\n",
    "    lab_options += sorted(labs)\n",
    "\n",
    "case_count = int(_summary_base[\"case\"].nunique()) if \"case\" in _summary_base.columns else 10\n",
    "max_cases_cap = max(10, min(80, case_count))\n",
    "\n",
    "summary_metric = widgets.Dropdown(options=metric_options, value=metric_options[0], description=\"Metric\")\n",
    "summary_hue = widgets.Dropdown(options=hue_options, value=\"(none)\", description=\"Hue\")\n",
    "summary_lab = widgets.Dropdown(options=lab_options, value=\"(all)\", description=\"Lab\")\n",
    "summary_tag = widgets.Text(value=\"\", description=\"Tag contains\")\n",
    "summary_max_cases = widgets.IntSlider(value=min(30, max_cases_cap), min=5, max=max_cases_cap, step=5, description=\"Max cases\")\n",
    "summary_label_max = widgets.IntSlider(value=32, min=10, max=80, step=2, description=\"Label max\")\n",
    "\n",
    "summary_out = widgets.Output()\n",
    "summary_controls = widgets.VBox([\n",
    "    widgets.HBox([summary_metric, summary_hue, summary_lab]),\n",
    "    widgets.HBox([summary_tag, summary_max_cases, summary_label_max]),\n",
    "])\n",
    "\n",
    "display(summary_controls, summary_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "27c2ed084f9745208d8135a90f2f9686",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def render_summary(*_):\n",
    "    with summary_out:\n",
    "        clear_output(wait=True)\n",
    "        metric = summary_metric.value\n",
    "        if not metric:\n",
    "            print(\"No metric selected.\")\n",
    "            return\n",
    "        filter_lab = None if summary_lab.value == \"(all)\" else summary_lab.value\n",
    "        filter_tag = summary_tag.value.strip() or None\n",
    "        max_cases = int(summary_max_cases.value) if summary_max_cases.value else None\n",
    "\n",
    "        try:\n",
    "            result = prepare_summary(\n",
    "                index,\n",
    "                metric,\n",
    "                filter_lab=filter_lab,\n",
    "                filter_tag=filter_tag,\n",
    "                max_cases=max_cases,\n",
    "            )\n",
    "        except Exception as exc:\n",
    "            print(f\"Summary error: {exc}\")\n",
    "            return\n",
    "\n",
    "        if result[\"df\"].empty:\n",
    "            print(\"No rows to plot after filtering.\")\n",
    "            return\n",
    "\n",
    "        display(result[\"summary_table\"])\n",
    "\n",
    "        hue_value = summary_hue.value\n",
    "        plot_summary(\n",
    "            result,\n",
    "            metric=metric,\n",
    "            hue_value=None if hue_value == \"(none)\" else hue_value,\n",
    "            label_max=int(summary_label_max.value),\n",
    "        )\n",
    "\n",
    "for widget in (summary_metric, summary_hue, summary_lab, summary_tag, summary_max_cases, summary_label_max):\n",
    "    widget.observe(render_summary, names=\"value\")\n",
    "\n",
    "render_summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "606b7dbdcf30452096315a4f5af11054",
   "source": [
    "## Case analysis: compare configurations (interactive)\n",
    "\n",
    "Select a case and compare configurations. Labels are shortened in the plots;\n",
    "full configuration strings appear in the table.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "7f585b61ef2f4ef1a17a842ce7b401c7",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "case_base = ensure_dataframe(index)\n",
    "\n",
    "_case_metric_candidates = [\"min\", \"p50\", \"p95\", \"p99\", \"p999\", \"max\", \"mean\"]\n",
    "case_metric_options = [m for m in _case_metric_candidates if m in case_base.columns]\n",
    "if not case_metric_options:\n",
    "    case_metric_options = [\"p50\"]\n",
    "\n",
    "case_options = []\n",
    "if \"case\" in case_base.columns:\n",
    "    case_options = sorted(case_base[\"case\"].dropna().astype(str).unique().tolist())\n",
    "if not case_options:\n",
    "    case_options = [\"\"]\n",
    "\n",
    "config_field_candidates = [\"pin_cpu\", \"tags\", \"iters\", \"warmup\", \"bench_args\"]\n",
    "config_field_options = [col for col in config_field_candidates if col in case_base.columns]\n",
    "if not config_field_options:\n",
    "    config_field_options = config_field_candidates\n",
    "\n",
    "case_name_widget = widgets.Dropdown(options=case_options, value=case_options[0], description=\"Case\")\n",
    "case_primary_metric = widgets.Dropdown(options=case_metric_options, value=case_metric_options[0], description=\"Metric\")\n",
    "case_metrics_widget = widgets.SelectMultiple(\n",
    "    options=case_metric_options,\n",
    "    value=tuple([m for m in [\"p50\", \"p95\", \"p99\", \"p999\"] if m in case_metric_options]) or (case_metric_options[0],),\n",
    "    description=\"Profile\",\n",
    ")\n",
    "case_config_fields = widgets.SelectMultiple(\n",
    "    options=config_field_options,\n",
    "    value=tuple(config_field_options),\n",
    "    description=\"Config fields\",\n",
    ")\n",
    "case_max_configs = widgets.IntSlider(value=8, min=2, max=20, step=1, description=\"Max configs\")\n",
    "case_label_max = widgets.IntSlider(value=40, min=10, max=80, step=2, description=\"Label max\")\n",
    "\n",
    "case_out = widgets.Output()\n",
    "case_controls = widgets.VBox([\n",
    "    widgets.HBox([case_name_widget, case_primary_metric, case_max_configs]),\n",
    "    widgets.HBox([case_metrics_widget, case_config_fields, case_label_max]),\n",
    "])\n",
    "\n",
    "display(case_controls, case_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "c51df1f5f969421087196dafff6ef440",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def render_case(*_):\n",
    "    with case_out:\n",
    "        clear_output(wait=True)\n",
    "        case_name = case_name_widget.value\n",
    "        primary_metric = case_primary_metric.value\n",
    "        metrics = list(case_metrics_widget.value)\n",
    "        config_cols = list(case_config_fields.value)\n",
    "        label_max = int(case_label_max.value)\n",
    "        max_configs = int(case_max_configs.value)\n",
    "\n",
    "        try:\n",
    "            result = prepare_case(\n",
    "                index,\n",
    "                case_name,\n",
    "                config_columns=config_cols,\n",
    "                primary_metric=primary_metric,\n",
    "                metrics=metrics,\n",
    "            )\n",
    "        except Exception as exc:\n",
    "            print(f\"Case analysis error: {exc}\")\n",
    "            return\n",
    "\n",
    "        df = result[\"df\"]\n",
    "        case_name = result[\"case_name\"]\n",
    "        if df.empty:\n",
    "            print(f\"No runs found for case: {case_name!r}\")\n",
    "            return\n",
    "\n",
    "        config_order = result[\"config_order\"]\n",
    "        label_map, _display_order = plot_case_distribution(\n",
    "            df,\n",
    "            config_order=config_order,\n",
    "            primary_metric=primary_metric,\n",
    "            unit_label=result[\"unit_label\"],\n",
    "            label_max=label_max,\n",
    "            title=f\"{case_name} by configuration ({primary_metric})\",\n",
    "        )\n",
    "\n",
    "        summary_table = result[\"summary_table\"]\n",
    "        if summary_table is not None:\n",
    "            table = summary_table.reset_index()\n",
    "            table[\"config_display\"] = table[\"config\"].map(label_map)\n",
    "            table = table[[\"config_display\", \"config\", \"run_count\", \"median\", \"min\", \"max\"]]\n",
    "            display(table)\n",
    "\n",
    "        metrics = [m for m in metrics if m in df.columns]\n",
    "        if metrics:\n",
    "            top_configs = config_order[:max_configs] if max_configs else config_order\n",
    "            profile = build_profile(df, configs=top_configs, metrics=metrics)\n",
    "            if len(profile):\n",
    "                profile[\"config_display\"] = profile[\"config\"].map(label_map)\n",
    "                plot_profile(\n",
    "                    profile,\n",
    "                    unit_label=result[\"unit_label\"],\n",
    "                    title=f\"{case_name} quantile profile (top {len(top_configs)})\",\n",
    "                )\n",
    "\n",
    "for widget in (\n",
    "    case_name_widget,\n",
    "    case_primary_metric,\n",
    "    case_metrics_widget,\n",
    "    case_config_fields,\n",
    "    case_max_configs,\n",
    "    case_label_max,\n",
    "):\n",
    "    widget.observe(render_case, names=\"value\")\n",
    "\n",
    "render_case()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latency-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}